{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Egeria Logo](https://raw.githubusercontent.com/odpi/egeria/main/assets/img/ODPi_Egeria_Logo_color.png)\n",
    "\n",
    "### Performance Suite Results\n",
    "# Crux Plugin Repository Connector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Following are detailed results for the Crux Connector's performance at various scales. In each set of results, the Crux Connector was tested under the following conditions (full details are available in the each results directory's `deployment` file):\n",
    "\n",
    "- Running through a Kubernetes pod with a single OMAG Platform running both the Performance Test Suite server and the Crux Plugin Repository\n",
    "- Running a co-located Bitnami Kafka (and Zookeeper) pod on the same Kubernetes node running the OMAG Platform\n",
    "- Resources allocated are a minimum of 2 cores to a maximum of 4 cores, and a minimum of 8GB memory to a maximum of 16GB memory\n",
    "\n",
    "### Versions\n",
    "\n",
    "Component | Version | Notes\n",
    "---|---|---\n",
    "Egeria | 2.10 | OMAG Platform, CTS, PTS\n",
    "Crux Plugin Repository Connector | 2.10 |\n",
    "Crux | 21.05-1.17.0-beta | Embedded in Crux Plugin Repository Connector\n",
    "RocksDB | 6.15.2 | Used for transaction log, document store and index store for Crux\n",
    "Kafka | 2.3.1 | Used for cohort event bus\n",
    "Lucene | 8.8.2 | Used for text indexing in Crux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Results locations\n",
    "\n",
    "Locations for the results (see subdirectories in the same location where this notebook resides to review the raw results themselves):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [\n",
    "    \"pts-05-02\",\n",
    "    \"pts-10-05\",\n",
    "    \"janus-05-02\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis and visualization methods\n",
    "\n",
    "The following defines methods necessary to parse, process and visualize the results, and must be run prior to the subsequent cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "def validateProfileResultsLocation(location):\n",
    "    profile_details_location = location + os.path.sep + \"profile-details\"\n",
    "    print(\"Validating profile-details location:\", profile_details_location)\n",
    "    if os.path.isdir(profile_details_location):\n",
    "        print(\" ... directory exists.\")\n",
    "    else:\n",
    "        print(\" ... ERROR: could not find this directory. Is the location specified correct?\")\n",
    "\n",
    "# Define the profile ordering\n",
    "profile_order=[\n",
    "    'Entity creation', 'Entity search', 'Relationship creation', 'Relationship search',\n",
    "    'Entity classification', 'Classification search', 'Entity update', 'Relationship update',\n",
    "    'Classification update', 'Entity undo', 'Relationship undo', 'Entity retrieval', 'Entity history retrieval',\n",
    "    'Relationship retrieval', 'Relationship history retrieval', 'Entity history search', 'Relationship history search',\n",
    "    'Graph queries', 'Graph history queries', 'Entity re-home', 'Relationship re-home', 'Entity declassify',\n",
    "    'Entity re-type', 'Relationship re-type', 'Entity re-identify', 'Relationship re-identify',\n",
    "    'Relationship delete', 'Entity delete', 'Entity restore', 'Relationship restore', 'Relationship purge',\n",
    "    'Entity purge'\n",
    "]\n",
    "\n",
    "# Given a profileResult.requirementResults object, parse all of its positiveTestEvidence\n",
    "# for discovered properties\n",
    "def parseProperties(df, repositoryName, requirementResults):\n",
    "    if (requirementResults is not None and 'positiveTestEvidence' in requirementResults):\n",
    "        print(\"Parsing properties for:\", requirementResults['name'], \"(\" + repositoryName + \")\")\n",
    "        data_array = []\n",
    "        for evidence in requirementResults['positiveTestEvidence']:\n",
    "            if ('propertyName' in evidence and 'propertyValue' in evidence):\n",
    "                data = {\n",
    "                    'repo': repositoryName,\n",
    "                    'property_name': evidence['propertyName'],\n",
    "                    'property_value': evidence['propertyValue']\n",
    "                }\n",
    "                data_array.append(data)\n",
    "        df = df.append(pd.read_json(json.dumps(data_array), orient='records'), ignore_index=True)\n",
    "    return df\n",
    "\n",
    "# Given a profileResult.requirementResults object, parse all of its positiveTestEvidence\n",
    "# and group the results by methodName\n",
    "def parseEvidence(df, repositoryName, requirementResults):\n",
    "    if (requirementResults is not None and 'positiveTestEvidence' in requirementResults):\n",
    "        print(\"Parsing evidence for:\", requirementResults['name'], \"(\" + repositoryName + \")\")\n",
    "        data_array = []\n",
    "        for evidence in requirementResults['positiveTestEvidence']:\n",
    "            if ('methodName' in evidence and 'elapsedTime' in evidence):\n",
    "                data = {\n",
    "                    'repo': repositoryName,\n",
    "                    'method_name': evidence['methodName'],\n",
    "                    'elapsed_time': evidence['elapsedTime'],\n",
    "                    'profile_name': requirementResults['name'],\n",
    "                    'test_case_id': evidence['testCaseId'],\n",
    "                    'assertion_id': evidence['assertionId']\n",
    "                }\n",
    "                data_array.append(data)\n",
    "        df = df.append(pd.read_json(json.dumps(data_array), orient='records'), ignore_index=True)\n",
    "    return df\n",
    "\n",
    "# Given a profile detail JSON file, retrieve all of its profileResult.requirementResults[] objects\n",
    "def parseRequirementResults(profileFile):\n",
    "    with open(profileFile) as f:\n",
    "        profile = json.load(f)\n",
    "    # This first case covers files retrieved via API\n",
    "    if ('profileResult' in profile and 'requirementResults' in profile['profileResult']):\n",
    "        return profile['profileResult']['requirementResults']\n",
    "    # This second case covers files created by the CLI client\n",
    "    elif ('requirementResults' in profile):\n",
    "        return profile['requirementResults']\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def getEnvironmentProfile(profileLocation):\n",
    "    detailsLocation = profileLocation + os.path.sep + \"profile-details\"\n",
    "    return detailsLocation + os.path.sep + \"Environment.json\"\n",
    "\n",
    "def parseEnvironmentDetailsIntoDF(df, profileFile, qualifier):\n",
    "    profileResults = parseRequirementResults(profileFile)\n",
    "    if profileResults is not None:\n",
    "        for result in profileResults:\n",
    "            df = parseProperties(df, qualifier, result)\n",
    "    return df\n",
    "\n",
    "# Retrieve a listing of all of the profile detail JSON files\n",
    "def getAllProfiles(profileLocation):\n",
    "    detailsLocation = profileLocation + os.path.sep + \"profile-details\"\n",
    "    _, _, filenames = next(os.walk(detailsLocation))\n",
    "    full_filenames = []\n",
    "    for filename in filenames:\n",
    "        full_filenames.append(detailsLocation + os.path.sep + filename)\n",
    "    return full_filenames\n",
    "\n",
    "# Parse all of the provided profile file's details into the provided dataframe\n",
    "def parseProfileDetailsIntoDF(df, profileFile, qualifier):\n",
    "    profileResults = parseRequirementResults(profileFile)\n",
    "    if profileResults is not None:\n",
    "        for result in profileResults:\n",
    "            df = parseEvidence(df, qualifier, result)\n",
    "    return df\n",
    "\n",
    "def plotMethod(df, methodName, remove_outliers=False, by_repo=False, by_assertion=False):\n",
    "    dfX = df[df['method_name'] == methodName]\n",
    "    if not dfX.empty:\n",
    "        if remove_outliers:\n",
    "            dfX = dfX[dfX['elapsed_time'].between(dfX['elapsed_time'].quantile(.00), dfX['elapsed_time'].quantile(.99))]\n",
    "        sns.set(font_scale=1.2)\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        fix, axs = plt.subplots(ncols=1, nrows=1, figsize=(18,9))\n",
    "        if by_repo:\n",
    "            # Display the repos within the method in alphabetical order for consistency\n",
    "            repos = dfX['repo'].unique()\n",
    "            figure = sns.histplot(ax=axs, data=dfX, x=\"elapsed_time\", hue=\"repo\",\n",
    "                                  hue_order=sorted(repos), kde=True, discrete=False)\n",
    "        if by_assertion:\n",
    "            # Display the assertions within the method in alphabetical order for consistency\n",
    "            assertions = dfX['assertion_id'].unique()\n",
    "            figure = sns.histplot(ax=axs, data=dfX, x=\"elapsed_time\", hue=\"assertion_id\",\n",
    "                                  hue_order=sorted(assertions), kde=True, discrete=False)\n",
    "        else:\n",
    "            figure = sns.histplot(ax=axs, data=dfX, x=\"elapsed_time\",\n",
    "                                  kde=True, discrete=False)\n",
    "        figure.set(xlabel=\"Elapsed time (ms)\")\n",
    "        figure.set_title(methodName)\n",
    "        display(fix)\n",
    "        plt.close(fix)\n",
    "\n",
    "def plotProfile(df, profileName, remove_outliers=False):\n",
    "    dfX = df[df['profile_name'] == profileName]\n",
    "    # Only attempt to plot if there is anything left in the dataframe\n",
    "    if not dfX.empty:\n",
    "        if remove_outliers:\n",
    "            # If we have been asked to remove outliers, drop anything outside the 2nd and 98th percentiles\n",
    "            dfX = dfX[dfX['elapsed_time'].between(dfX['elapsed_time'].quantile(.02), dfX['elapsed_time'].quantile(.98))]\n",
    "        sns.set(font_scale=1.2)\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        fix, axs = plt.subplots(ncols=1, nrows=1, figsize=(18,9))\n",
    "        # Display the methods within the profile in alphabetical order for consistency\n",
    "        methods = dfX['method_name'].unique()\n",
    "        figure = sns.histplot(ax=axs, data=dfX, x=\"elapsed_time\", hue=\"method_name\",\n",
    "                              hue_order=sorted(methods), kde=True, discrete=False)\n",
    "        figure.set(xlabel=\"Elapsed time (ms)\")\n",
    "        figure.set_title(profileName)\n",
    "        figure.get_legend().set(title='Method')\n",
    "        display(fix)\n",
    "        plt.close(fix)\n",
    "\n",
    "def slowestRunning(df, num=10, methodName=None):\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    if methodName:\n",
    "        df = df[df['method_name'] == methodName]\n",
    "    display(df.sort_values(by=['elapsed_time'], ascending=False).groupby('method_name').head(num))\n",
    "\n",
    "def compareProfiles(df, profileName, left, right, remove_outliers=False):\n",
    "    dfX = df[df['profile_name'] == profileName]\n",
    "    # Only attempt to plot if there is anything left in the dataframe\n",
    "    if not dfX.empty:\n",
    "        if remove_outliers:\n",
    "            # If we have been asked to remove outliers, drop anything outside the 2nd and 98th percentiles\n",
    "            dfX = dfX[dfX['elapsed_time'].between(dfX['elapsed_time'].quantile(.02), dfX['elapsed_time'].quantile(.98))]\n",
    "        sns.set(font_scale=1.2)\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        fix, axs = plt.subplots(ncols=1, nrows=1, figsize=(18,9))\n",
    "        # Display the methods within the profile in alphabetical order for consistency\n",
    "        methods = dfX['method_name'].unique()\n",
    "        figure = sns.violinplot(x=\"method_name\", y=\"elapsed_time\", ax=axs, hue=\"repo\",\n",
    "                                hue_order=[left, right], split=True, scale='count',\n",
    "                                inner='quartile', cut=0, data=dfX)\n",
    "        # If there are more than 4 methods in the profile, rotate them so they are still readable\n",
    "        if (len(methods) > 4):\n",
    "            figure.set_xticklabels(figure.get_xticklabels(), rotation=10)\n",
    "        figure.set(xlabel=\"Method name\", ylabel=\"Elapsed time (ms)\")\n",
    "        figure.set_title(profileName + ' comparison')\n",
    "        figure.get_legend().set(title='Test')\n",
    "        display(fix)\n",
    "        plt.close(fix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## instancesPerType=5, maxSearchResults=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results0 = results[0]\n",
    "\n",
    "validateProfileResultsLocation(results0)\n",
    "files = getAllProfiles(results0)\n",
    "\n",
    "df1 = pd.DataFrame({'repo': [], 'method_name': [], 'elapsed_time': [], 'profile_name': [], 'test_case_id': [], 'assertion_id': []})\n",
    "dfEnv = None\n",
    "\n",
    "for profile_file in files:\n",
    "    df1 = parseProfileDetailsIntoDF(df1, profile_file, results0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results0_env = getEnvironmentProfile(results0)\n",
    "env0 = pd.DataFrame({'repo': [], 'property_name': [], 'property_value': []})\n",
    "env0 = parseEnvironmentDetailsIntoDF(env0, results0_env, results0)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "display(env0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full response time profiles\n",
    "\n",
    "The following plots the response times of each method within each profile in full, including any extreme / outlying values. (As this is rendering 30+ detailed visualizations it may take a little time to complete.)\n",
    "\n",
    "From these visualizations, we can quickly see the range of response times for a given method and where there values are more typical (high peaks) than not (long tails). This allows us to quickly assess two important areas:\n",
    "\n",
    "1. Any methods that appear to consistently run for a longer time than we may want or expect.\n",
    "1. Any particular combination of parameters that may cause a method that in most cases runs quickly to in certain cases run particularly slowly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for profile in profile_order:\n",
    "    plotProfile(df1, profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "From the plots above we can see that most methods have very high peaks towards the left of the graph: indicating that the vast majority of the executions of that method have response times in that range. However, there are a number of cases where various methods run for much longer than this usual response time (even up to several seconds).\n",
    "\n",
    "To see whether these are rare outliers, we may want to re-plot the profiles again: this time ignoring the slowest 1% of the values in the response times. Stated differently, this will show the response time of 99% of the method calls: if there is a consistently-slow combination of parameters, we will expect it to show up as part of this 99% cut-off point in these plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Typical\" response time profiles\n",
    "\n",
    "The following plots the response times of each method within each profile focusing only on the typical values -- specifically removing any outliers within the top and bottom 2% of the response times. From these visualizations, we can quickly see the \"typical\" response times for a given method, keeping in mind that we are ignoring the outlying extreme values here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for profile in profile_order:\n",
    "    plotProfile(df1, profile, remove_outliers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without the outliers, we can more clearly see the typical distribution of each method's response times: and that in most cases (99% of the methods' executions) the response times are sub-second (in most cases even less than 250ms).\n",
    "\n",
    "We can also see that there are however a few exceptions to this -- the various graph queries all have very long tails that suggest there are a number of examples of very long-running methods. In addition, various write operations also have long tails that appear to occur relatively infrequently but nonetheless extend to around 1 second within the 99% range.\n",
    "\n",
    "We can start by looking at the top-10 slowest response times for each of these individual methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slowest = ['updateEntityProperties', 'getEntityNeighborhood', 'getLinkingEntities', 'getRelatedEntities', 'getRelationshipsForEntity']\n",
    "for slow in slowest:\n",
    "    slowestRunning(df1, num=10, methodName=slow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that each of these top-10 slowest results for these various methods are similar, and the result of the method running against a different set of parameters (for example, against different types of instances). This would suggest that these response times were not simply a one-off or pseudo-random occurrence that could have been caused by something like a garbage collection pause, but that there is more likely to be some fundamental underlying reason for this particular performance. To find out more, we need to delve back into the repository connector itself with deeper profiling of these particular combinations of parameters for each method to see if there is some further optimization that can be done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing results\n",
    "\n",
    "Up to this point, we have done some analysis of the performance of a single set of volume parameters. However, we may also be interested in comparing and contrasting these results with additional volume parameters to investigate the scalability of the connector as the volume of metadata within the repository grows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results1 = results[1]\n",
    "\n",
    "validateProfileResultsLocation(results1)\n",
    "files = getAllProfiles(results1)\n",
    "\n",
    "for profile_file in files:\n",
    "    df1 = parseProfileDetailsIntoDF(df1, profile_file, results1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### instancesPerType=10, maxSearchResults=5 details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results1_env = getEnvironmentProfile(results1)\n",
    "env1 = pd.DataFrame({'repo': [], 'property_name': [], 'property_value': []})\n",
    "env1 = parseEnvironmentDetailsIntoDF(env1, results1_env, results1)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "display(env1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ipt=5, msr=2 compared to ipt=10, msr=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for profile in profile_order:\n",
    "    compareProfiles(df1, profile, results0, results1, remove_outliers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "For the most part, the performance for each method is comparable -- even though we have doubled the number of instances involved (from 4470 to 8940) and the number of results returned by each page of a search (from 2 to 5).\n",
    "\n",
    "The notable exceptions are the various search methods and the graph queries, in particular `getRelatedEntities` and `getLinkingEntities` which we can see have a significant additional peak. This may be understandable, given the additional number of instances is likely to equate to a significant increase in the number of relationships and linked entities that these methods will retrieve in the higher volume environment (since these methods do not page results, but retrieve all relationships and entities involved)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other repositories\n",
    "\n",
    "We may also want to do some comparative analysis between repositories. The following looks at results from the JanusGraph repository at the same volume parameters to compare and contrast the relative performance of the two repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results2 = results[2]\n",
    "\n",
    "validateProfileResultsLocation(results2)\n",
    "files = getAllProfiles(results2)\n",
    "\n",
    "for profile_file in files:\n",
    "    df1 = parseProfileDetailsIntoDF(df1, profile_file, results2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results2_env = getEnvironmentProfile(results2)\n",
    "env2 = pd.DataFrame({'repo': [], 'property_name': [], 'property_value': []})\n",
    "env2 = parseEnvironmentDetailsIntoDF(env2, results2_env, results1)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "display(env2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for profile in profile_order:\n",
    "    compareProfiles(df1, profile, results0, results2, remove_outliers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "Here we can see that in _almost all_ cases, based on their default configurations only, the Crux repository connector is faster than the JanusGraph repository connector:\n",
    "\n",
    "- The Crux connector appears to be significantly faster (~3-4x) with write operations (create, update, delete, purge, restore, re-identify, etc)\n",
    "- The Crux connector also appears to be significantly faster for most search operations\n",
    "- For some operations (i.e. the graph queries) we did not even run them under JanusGraph due to each per-type test not completing after more than 3 hours (vs. Crux's few seconds for the same tests, at the same volume).\n",
    "\n",
    "Only the retrieval methods are roughly equivalent between the two repositories. Of course, there may be further optimisations possible with either or both repositories to further improve their performance for certain aspects: this is only comparing the default configuration of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotMethod(df1, \"findEntities\", by_repo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slowestRunning(df1[df1['repo'] == 'janus-05-02'], num=10, methodName='findEntities')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly we can see that some predicted suspects like `Referenceable` and `OpenMetadataRoot` are particularly slow-performing; however, these are not alone given `UserAccessDirectory`, `VerificationPoint`, and `UserProfileManager` each also demonstrate response times that exceed 5 seconds (and are closely followed by a number of others that come close to 5 seconds).\n",
    "\n",
    "Instead of the metadata type being the distinguishing factor, it appears it is the search parameters that are most important:\n",
    "\n",
    "- For `Referenceable` and `OpenMetadataRoot` the slow-running examples come from the `repository-entity-retrieval-performance` set of tests: these run `findEntities` with only a type GUID as a filter.\n",
    "- All of the other slowest-running examples come from the `repository-entity-classification-performance` set of tests: where `findEntities` is called with a classification criteria to retrieve a limited number of results.\n",
    "\n",
    "It would therefore appear that the JanusGraph repository connector's ability to search based on classification and to search based only on a very abstract supertype is significantly slower than the Crux repository connector's ability to do the same searches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
