{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Egeria Logo](https://raw.githubusercontent.com/odpi/egeria/main/assets/img/ODPi_Egeria_Logo_color.png)\n",
    "\n",
    "### Performance Suite Results\n",
    "# Crux Plugin Repository Connector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating medians\n",
    "\n",
    "This notebook simply calculates the medians for every method executed by the PTS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Results locations\n",
    "\n",
    "Locations for the results (see subdirectories in the same location where this notebook resides to review the raw results themselves):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [\n",
    "    \"pts-05-02\",\n",
    "    \"janus-05-02\",\n",
    "    \"pts-10-05\",\n",
    "    \"janus-10-05\",\n",
    "    \"pts-20-10\",\n",
    "    \"janus-20-10\",\n",
    "    \"pts-40-10\",\n",
    "    \"janus-40-10\",\n",
    "    \"pts-80-10\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing and analysis methods\n",
    "\n",
    "The following defines methods necessary to parse and process the results, and must be run prior to the subsequent cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "def validateProfileResultsLocation(location):\n",
    "    profile_details_location = location + os.path.sep + \"profile-details\"\n",
    "    print(\"Validating profile-details location:\", profile_details_location)\n",
    "    if os.path.isdir(profile_details_location):\n",
    "        print(\" ... directory exists.\")\n",
    "    else:\n",
    "        print(\" ... ERROR: could not find this directory. Is the location specified correct?\")\n",
    "\n",
    "# Define the profile ordering\n",
    "profile_order=[\n",
    "    'Entity creation', 'Entity search', 'Relationship creation', 'Relationship search',\n",
    "    'Entity classification', 'Classification search', 'Entity update', 'Relationship update',\n",
    "    'Classification update', 'Entity undo', 'Relationship undo', 'Entity retrieval', 'Entity history retrieval',\n",
    "    'Relationship retrieval', 'Relationship history retrieval', 'Entity history search', 'Relationship history search',\n",
    "    'Graph queries', 'Graph history queries', 'Entity re-home', 'Relationship re-home', 'Entity declassify',\n",
    "    'Entity re-type', 'Relationship re-type', 'Entity re-identify', 'Relationship re-identify',\n",
    "    'Relationship delete', 'Entity delete', 'Entity restore', 'Relationship restore', 'Relationship purge',\n",
    "    'Entity purge'\n",
    "]\n",
    "\n",
    "# Given a profileResult.requirementResults object, parse all of its positiveTestEvidence\n",
    "# and group the results by methodName\n",
    "def parseEvidence(df, repositoryName, requirementResults):\n",
    "    if (requirementResults is not None and 'positiveTestEvidence' in requirementResults):\n",
    "        print(\"Parsing evidence for:\", requirementResults['name'], \"(\" + repositoryName + \")\")\n",
    "        data_array = []\n",
    "        for evidence in requirementResults['positiveTestEvidence']:\n",
    "            if ('methodName' in evidence and 'elapsedTime' in evidence):\n",
    "                data = {\n",
    "                    'repo': repositoryName,\n",
    "                    'method_name': evidence['methodName'],\n",
    "                    'elapsed_time': evidence['elapsedTime'],\n",
    "                    'profile_name': requirementResults['name'],\n",
    "                    'test_case_id': evidence['testCaseId'],\n",
    "                    'assertion_id': evidence['assertionId']\n",
    "                }\n",
    "                data_array.append(data)\n",
    "        df = df.append(pd.read_json(json.dumps(data_array), orient='records'), ignore_index=True)\n",
    "    return df\n",
    "\n",
    "# Given a profile detail JSON file, retrieve all of its profileResult.requirementResults[] objects\n",
    "def parseRequirementResults(profileFile):\n",
    "    with open(profileFile) as f:\n",
    "        profile = json.load(f)\n",
    "    # This first case covers files retrieved via API\n",
    "    if ('profileResult' in profile and 'requirementResults' in profile['profileResult']):\n",
    "        return profile['profileResult']['requirementResults']\n",
    "    # This second case covers files created by the CLI client\n",
    "    elif ('requirementResults' in profile):\n",
    "        return profile['requirementResults']\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Retrieve a listing of all of the profile detail JSON files\n",
    "def getAllProfiles(profileLocation):\n",
    "    detailsLocation = profileLocation + os.path.sep + \"profile-details\"\n",
    "    _, _, filenames = next(os.walk(detailsLocation))\n",
    "    full_filenames = []\n",
    "    for filename in filenames:\n",
    "        full_filenames.append(detailsLocation + os.path.sep + filename)\n",
    "    return full_filenames\n",
    "\n",
    "# Parse all of the provided profile file's details into the provided dataframe\n",
    "def parseProfileDetailsIntoDF(df, profileFile, qualifier):\n",
    "    profileResults = parseRequirementResults(profileFile)\n",
    "    if profileResults is not None:\n",
    "        for result in profileResults:\n",
    "            df = parseEvidence(df, qualifier, result)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'repo': [], 'method_name': [], 'elapsed_time': [], 'profile_name': [], 'test_case_id': [], 'assertion_id': []})\n",
    "\n",
    "for result_name in results:\n",
    "    validateProfileResultsLocation(result_name)\n",
    "    files = getAllProfiles(result_name)\n",
    "    dfEnv = None\n",
    "    for profile_file in files:\n",
    "        df1 = parseProfileDetailsIntoDF(df1, profile_file, result_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header  = \"Profile | Method\"\n",
    "breaker = \"---|---\"\n",
    "for result_name in results:\n",
    "    header  += \" | \" + result_name\n",
    "    breaker += \"|---\"\n",
    "print(header)\n",
    "print(breaker)\n",
    "for profile_name in profile_order:\n",
    "    groups = df1[df1['profile_name'] == profile_name].groupby(['repo', 'method_name'], as_index=False).median()\n",
    "    methods = sorted(groups['method_name'].unique())\n",
    "    grouped_results = []\n",
    "    for result in results:\n",
    "        grouped_results.append(groups[groups['repo'] == result])\n",
    "    index = 0\n",
    "    for method in methods:\n",
    "        row = \" ... | \"\n",
    "        if index == 0:\n",
    "            row = profile_name + \" | \"\n",
    "        row += method\n",
    "        for result in grouped_results:\n",
    "            value = result.loc[result['method_name'] == method, 'elapsed_time']\n",
    "            if value.empty:\n",
    "                row += \" | --\"\n",
    "            else:\n",
    "                row += \" | \" + str(value.values[0])\n",
    "        print(row)\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation\n",
    "\n",
    "Plot the results for visual comparison purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import median\n",
    "\n",
    "cruxOnly = [\n",
    "    'Entity undo',\n",
    "    'Relationship undo',\n",
    "    'Entity history retrieval',\n",
    "    'Relationship history retrieval',\n",
    "    'Entity history search',\n",
    "    'Relationship history search',\n",
    "    'Graph queries',\n",
    "    'Graph history queries'\n",
    "]\n",
    "\n",
    "alwaysSkip = [\n",
    "    'Graph queries',\n",
    "    'Graph history queries'\n",
    "]\n",
    "\n",
    "def compareResults(dfX, all_methods=False):\n",
    "    if not all_methods:\n",
    "        # Skip the profiles that are only implemented in the Crux repository\n",
    "        dfX = dfX[~dfX['profile_name'].isin(cruxOnly)]\n",
    "    else:\n",
    "        dfX = dfX[~dfX['profile_name'].isin(alwaysSkip)]\n",
    "    sns.set(font_scale=1.2)\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    method_order = []\n",
    "    for profile_name in profile_order:\n",
    "        if (all_methods and profile_name not in alwaysSkip) or profile_name not in cruxOnly:\n",
    "            profile_details = df1[df1['profile_name'] == profile_name]\n",
    "            for method_name in sorted(profile_details['method_name'].unique()):\n",
    "                method_order.append(method_name)\n",
    "    fix, axs = plt.subplots(ncols=1, nrows=1, figsize=(18,18))\n",
    "    figure = None\n",
    "    if all_methods:\n",
    "        figure = sns.pointplot(x=\"elapsed_time\", y=\"method_name\", hue=\"repo\",\n",
    "                               data=dfX, estimator=median, ax=axs,\n",
    "                               hue_order=sorted(results),\n",
    "                               plot_kws=dict(alpha=0.5),\n",
    "                               order=method_order)\n",
    "    else:\n",
    "        figure = sns.pointplot(x=\"elapsed_time\", y=\"method_name\", hue=\"repo\",\n",
    "                               data=dfX, estimator=median, ax=axs,\n",
    "                               hue_order=sorted(results),\n",
    "                               markers=['x', 'x', 'x', 'x', 'o', 'o', 'o', 'o', 'o'],\n",
    "                               linestyles=['--', '--', '--', '--', '-', '-', '-', '-', '-'],\n",
    "                               plot_kws=dict(alpha=0.5),\n",
    "                               order=method_order)\n",
    "    figure.set(xlabel='Elapsed time (ms)', ylabel='')\n",
    "    figure.set_title(\"Typical execution time comparison\")\n",
    "    figure.get_legend().set(title=\"Repo and volume\")\n",
    "    plt.setp(figure.collections, alpha=0.5)\n",
    "    display(fix)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('comparison.svg')\n",
    "    plt.close(fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compareResults(df1, all_methods=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
